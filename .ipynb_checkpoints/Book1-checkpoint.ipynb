{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 处理照片Exif信息\n",
    "\n",
    "import exifread\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "# 读取EXIF信息\n",
    "def getPhotoExif(filename):\n",
    "    fd = open(filename,'rb')\n",
    "    tags = exifread.process_file(fd)\n",
    "    fd.close()\n",
    "    return(tags)\n",
    "\n",
    "def movePhoto(path,dst):\n",
    "    n = 1\n",
    "    m = 0\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for filename in files:\n",
    "            filename = os.path.join(root,filename)\n",
    "            f,ext = os.path.splitext(filename)\n",
    "            if ext.lower() not in ('.jpg','.png','.mp4','.gif'):\n",
    "                continue\n",
    "            tags = getPhotoExif(filename)\n",
    "            #print(\"----------------------------------------------------------------\")            \n",
    "            try:\n",
    "                date = str(tags['EXIF DateTimeOriginal']).replace(\":\",\"-\")[:10]\n",
    "                #print(n,\"文件:\",filename,\"相机:\",tags['Image Model'],\"，拍摄时间：\",date)\n",
    "                pwd = root + \"\\\\\" + date\n",
    "                year = date[0:4]\n",
    "                yearpath = dst+\"\\\\\"+year\n",
    "\n",
    "                if not os.path.exists(yearpath):\n",
    "                    os.mkdir(yearpath)\n",
    "                daypath = yearpath+\"\\\\\"+date\n",
    "\n",
    "                if not os.path.exists(daypath):\n",
    "                    os.mkdir(daypath)\n",
    "                shutil.move(filename,daypath)\n",
    "                print(n,filename+\"  ----->  \"+daypath)\n",
    "                n = n + 1\n",
    "            except:\n",
    "                print(\"照片\",filename,\"没有EXIF数据\")\n",
    "                m = m + 1\n",
    "                pass\n",
    "    \n",
    "    print(\"共移动\" ,n-1,\"文件，\",m,\"个文件未移动\")\n",
    "\n",
    "def main():\n",
    "    \n",
    "    msg = \"\"\"使用方法：\n",
    "        python movePhoto.py 源文件夹 目标文件夹\n",
    "        \"\"\"\n",
    "    \n",
    "    if len(sys.argv) < 3:\n",
    "        print(msg)\n",
    "    else:\n",
    "        movePhoto(sys.argv[1],sys.argv[2])\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "\n",
    "word = input(\"请输入要搜索的图片：\")\n",
    "if not os.path.exists('c:/Temp/' + word):\n",
    "    os.mkdir('c:/Temp/' + word)\n",
    "\n",
    "url = \"https://image.baidu.com/search/index?tn=baiduimage&ipn=r&ct=201326592\\\n",
    "&cl=2&lm=-1&st=-1&fm=result&fr=&sf=1&fmq=1540822430688_R&pv=&ic=0&nc=1&z=&se=1\\\n",
    "&showtab=0&fb=0&width=&height=&face=0&istype=2&ie=utf-8&word=\" + word\n",
    "\n",
    "header = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:63.0) \\\n",
    "Gecko/20100101 Firefox/63.0\"}\n",
    "\n",
    "r = requests.get(url, headers=header, timeout=3).content.decode(\"utf-8\")\n",
    "# r.encoding = \"utf-8\"\n",
    "# r = r.text\n",
    "\n",
    "img_list = re.findall('\"objURL\":\"(.*?)\",', r)\n",
    "# print(img_list)\n",
    "\n",
    "for img in img_list:\n",
    "    print(img)\n",
    "    end = re.search('(.jpg|.png|.gif|.jpeg)$', img)\n",
    "    if end == None:\n",
    "        img = img +'.jpg'\n",
    "\n",
    "    path = re.sub('\\?|\\/', '', img[-10:])\n",
    "    # print(path)\n",
    "    try:\n",
    "        with open('c:/Temp/' + word +'/{}'.format(path), 'ab') as f:\n",
    "            ret = requests.get(img, headers=header, timeout=3)\n",
    "            f.write(ret.content)\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import re\n",
    "import time\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "browser.get('https://www.csdn.net/')\n",
    "browser.implicitly_wait(10)\n",
    "\n",
    "i = 0\n",
    "for i in range(5):###设置下拉5次，如果想获取更多信息，增加下拉次数即可\n",
    "    browser.execute_script('window.scrollTo(0, document.body.scrollHeight)')##下拉，execute_script可以将进度条下拉到最底部\n",
    "    time.sleep(5)##睡眠一下，防止网络延迟，卡顿等\n",
    "\n",
    "data = []\n",
    "pattern = re.compile('<li.*?blog\".*?>.*?title\">.*?<a.*?>(.*?)</a>.*?<dd.*?name\">.*?<a.*?blank\">(.*?)</a>'\n",
    "                     '.*?<span.*?num\">(.*?)</span>.*?text\">(.*?)</span>.*?</li>',re.S)\n",
    "items = re.findall(pattern,browser.page_source)##这里网页源代码为下拉5次后的代码\n",
    "for item in items:\n",
    "    data.append({\n",
    "        'Title':item[0].strip(),\n",
    "        'Author' : item[1].strip(),\n",
    "        'ReadNum' : item[2] + item[3]\n",
    "    })\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from _md5 import md5\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# 获取完整网页源码 \n",
    "def get_one_html(url): \n",
    "    browser = webdriver.Phantomjs()\n",
    "    browser.get(url) \n",
    "    p_source = browser.page_source\n",
    "    browser.close() \n",
    "    return p_source \n",
    "\n",
    "# 用正则表达式提取链接并且返回URL \n",
    "def parse_html(html): \n",
    "    pattern = re.compile('<li.*?<img src=\"(.*?)\".*?/>.*?</li>', re.S)\n",
    "    urls = re.findall(pattern, html)\n",
    "    print(urls)\n",
    "    return urls\n",
    "\n",
    "# 下载图片，注意的是文件写入的是二进制，并且文件名使用MD5生成固定长度的十六进制，防止文件名冲突，index表示页数 \n",
    "def download_imgs(urls):\n",
    "    print('one page')\n",
    "    for index, url in enumerate(urls): \n",
    "        response = requests.get(url) \n",
    "        content = response.content\n",
    "        file_path = '{0}\\\\{1}{2}{3}'.format('F:\\\\jdpics', md5(content).hexdigest(), index, '.jpg')\n",
    "        with open(file_path, 'wb') as f: \n",
    "            f.write(content) \n",
    "            print('当前页面爬取完成')\n",
    "\n",
    "# 构造一个可以变化的URL，并下载\n",
    "def main(i): \n",
    "    url = 'http://jandan.net/ooxx/page-'+str(i)+'#comments' \n",
    "    html = get_one_html(url) \n",
    "    urls = parse_html(html)\n",
    "    download_imgs(urls) \n",
    "    \n",
    "# 实现一个多进程爬虫 \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    # for i in range(1, 85):\n",
    "    #     url = 'http://jandan.net/ooxx/page-'+str(i)+'#comments'\n",
    "    #     main(url)\n",
    "    pool = Pool()\n",
    "    pool.map(main, [i for i in range(1, 85)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "word = \"毛泽东\"\n",
    "\n",
    "url = \"https://image.baidu.com/search/index?tn=baiduimage&ipn=r&ct=201326592\\\n",
    "&cl=2&lm=-1&st=-1&fm=result&fr=&sf=1&fmq=1540822430688_R&pv=&ic=0&nc=1&z=&se=1\\\n",
    "&showtab=0&fb=0&width=&height=&face=0&istype=2&ie=utf-8&word=\" + word\n",
    "\n",
    "header = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:63.0) \\\n",
    "Gecko/20100101 Firefox/63.0\"}\n",
    "\n",
    "browser = webdriver.Firefox()\n",
    "browser.get(url)\n",
    "browser.implicitly_wait(10)\n",
    "\n",
    "i = 0\n",
    "for i in range(5):# 设置下拉5次\n",
    "    browser.execute_script('window.scrollTo(0, document.body.scrollHeight)')# 下拉到最底部\n",
    "    time.sleep(5)# 防止网络延迟，卡顿等\n",
    "\n",
    "r = browser.page_source\n",
    "browser.close()\n",
    "print(r)\n",
    "\n",
    "img_list = re.findall('\"objURL\":\"(.*?)\",', r)\n",
    "print(img_list)\n",
    "\n",
    "for img in img_list:\n",
    "#    print(img)\n",
    "\n",
    "'''\n",
    "    path = re.sub('\\?|\\/', '', img[-10:])\n",
    "\n",
    "    try:\n",
    "        with open('c:/Temp/' + word +'/{}'.format(path), 'ab') as f:\n",
    "            ret = requests.get(img, headers=header, timeout=30)\n",
    "            f.write(ret.content)\n",
    "    except Exception:\n",
    "        pass\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "class TaobaoSpider(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.url = \"https://rate.tmall.com/list_detail_rate.htm?itemId=559979448696\\\n",
    "&spuId=349188091&sellerId=2646031546&order=3&currentPage=\"\n",
    "        self.page = 1\n",
    "        self.headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:63.0) \\\n",
    "Gecko/20100101 Firefox/63.0\"}\n",
    "        \n",
    "    def start_request(self):\n",
    "        for i in range(10):\n",
    "            print(\"正在爬取第%d页...--------------------------------------------------------------------------\" % self.page)\n",
    "            content = requests.get(url=self.url + str(self.page), headers=self.headers).content.decode()\n",
    "            self.content_re(content)\n",
    "            self.page += 1\n",
    "    \n",
    "    def content_re(self, content):\n",
    "        ratedate = re.findall(r'\"rateDate\":\"(.*?)\",',content)\n",
    "        ratecontent = re.findall(r'\"rateContent\":\"(.*?)\",', content)\n",
    "        for rd, rc in zip(ratedate,ratecontent):\n",
    "            print(\"【\"+rd+\"】\", rc)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    spider = TaobaoSpider()\n",
    "    spider.start_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from lxml import etree\n",
    "\n",
    "class ZbSpider(object):\n",
    "    def __init__(self):\n",
    "        self.url = \"http://www.ccgp-qinghai.gov.cn/jilin/zbxxController.form?\"\n",
    "        self.headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:63.0) \\\n",
    "Gecko/20100101 Firefox/63.0\"}\n",
    "        self.page = 0\n",
    "        \n",
    "    def start_request(self):\n",
    "        lx = input(\"请输入公告类型（1.省级 2.州市县级）：\")\n",
    "        pg = input(\"请输入爬取页数：\")\n",
    "        for self.page in range(int(pg)):\n",
    "            print(\"正在爬取中标公告第【%d】页...-----------------------------\" % self.page)\n",
    "            html = requests.get(self.url+\"declarationType=W\"+\"&type=\"+lx+\"&pageNo=\"+\n",
    "str(self.page), headers=self.headers).content.decode()\n",
    "            self.zbxx(html)\n",
    "        \n",
    "        self.page = 0\n",
    "        for self.page in range(int(pg)):\n",
    "            print(\"正在爬取废流标公告第【%d】页...---------------------------\" % self.page)\n",
    "            html = requests.get(self.url+\"declarationType=F\"+\"&type=\"+lx+\"&pageNo=\"+\n",
    "str(self.page), headers=self.headers).content.decode()\n",
    "            self.zbxx(html)\n",
    "\n",
    "    #中标公告信息获取\n",
    "    \n",
    "    def zbxx(self, html):\n",
    "        #stitle = re.findall(r'title=\"(.*?)\"', html)    #公告标题\n",
    "        surl = re.findall(r'\" href=\"(.*?)\">', html)\n",
    "        for su in surl:\n",
    "            #实际公告页面在框架内，去除框架页多余字符，合成实际url\n",
    "            sggurl = su.replace('ftl/jilin/noticeDetail.jsp?htmlURL=','')\n",
    "            print(sggurl)\n",
    "            sgg = requests.get(sggurl, headers=self.headers).content\n",
    "            s = etree.HTML(sgg)\n",
    "            name = s.xpath('/html/body/div/table/tbody/tr[2]/td[2]/p/span/text()')\n",
    "            date = s.xpath('/html/body/div/table/tbody/tr[8]/td[2]/p/span/text()')\n",
    "            man = s.xpath('/html/body/div/table/tbody/tr[13]/td[2]/p/span/text()')\n",
    "            print('【'+date[0]+'】',name[0],'【'+man[0]+'】')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    zb = ZbSpider()\n",
    "    zb.start_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入爬取页数：7\n",
      "<class 'int'>\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#lx = input(\"请输入公告类型（1.省级 2.州市县级）：\")\n",
    "pg = input(\"请输入爬取页数：\")\n",
    "#print(type(lx))\n",
    "print(type(int(pg)))\n",
    "#n=\n",
    "for i in range(int(pg)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "url = \"http://www.ccgp-qinghai.gov.cn/html/2018/11/8/W062700.html\"\n",
    "\n",
    "header = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:63.0) \\\n",
    "Gecko/20100101 Firefox/63.0\"}\n",
    "\n",
    "r = requests.get(url, headers=header).content#.decode(\"gb2312\")\n",
    "s = etree.HTML(r)\n",
    "name = s.xpath('/html/body/div/table/tbody/tr[2]/td[2]/p/span/text()')\n",
    "date = s.xpath('/html/body/div/table/tbody/tr[8]/td[2]/p/span/text()')\n",
    "man = s.xpath('/html/body/div/table/tbody/tr[13]/td[2]/p/span/text()')\n",
    "print(name[0],date[0],man[0])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
