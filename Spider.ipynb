{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "\n",
    "word = input(\"请输入要搜索的图片：\")\n",
    "if not os.path.exists('c:/Temp/' + word):\n",
    "    os.mkdir('c:/Temp/' + word)\n",
    "\n",
    "url = \"https://image.baidu.com/search/index?tn=baiduimage&ipn=r&ct=201326592\\\n",
    "&cl=2&lm=-1&st=-1&fm=result&fr=&sf=1&fmq=1540822430688_R&pv=&ic=0&nc=1&z=&se=1\\\n",
    "&showtab=0&fb=0&width=&height=&face=0&istype=2&ie=utf-8&word=\" + word\n",
    "\n",
    "header = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:63.0) \\\n",
    "Gecko/20100101 Firefox/63.0\"}\n",
    "\n",
    "r = requests.get(url, headers=header, timeout=3).content.decode(\"utf-8\")\n",
    "\n",
    "img_list = re.findall(r'\"objURL\":\"(.*?)\",', r)\n",
    "\n",
    "for img in img_list:\n",
    "    print(img)\n",
    "    end = re.search(r'(.jpg|.png|.gif|.jpeg)$', img)\n",
    "    if end == None:\n",
    "        img = img +'.jpg'\n",
    "\n",
    "    path = re.sub('\\?|\\/', '', img[-10:])\n",
    "\n",
    "    try:\n",
    "        with open('c:/Temp/' + word +'/{}'.format(path), 'ab') as f:\n",
    "            ret = requests.get(img, headers=header, timeout=3)\n",
    "            f.write(ret.content)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _md5 import md5\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# 获取完整网页源码 \n",
    "def get_one_html(url): \n",
    "    browser = webdriver.Phantomjs()\n",
    "    browser.get(url) \n",
    "    p_source = browser.page_source\n",
    "    browser.close() \n",
    "    return p_source \n",
    "\n",
    "# 用正则表达式提取链接并且返回URL \n",
    "def parse_html(html): \n",
    "    pattern = re.compile(r'<li.*?<img src=\"(.*?)\".*?/>.*?</li>', re.S)\n",
    "    urls = re.findall(pattern, html)\n",
    "    print(urls)\n",
    "    return urls\n",
    "\n",
    "# 下载图片，注意的是文件写入的是二进制，并且文件名使用MD5生成固定长度的十六进制，防止文件名冲突，index表示页数 \n",
    "def download_imgs(urls):\n",
    "    print('one page')\n",
    "    for index, url in enumerate(urls): \n",
    "        response = requests.get(url) \n",
    "        content = response.content\n",
    "        file_path = '{0}\\\\{1}{2}{3}'.format('E:\\\\jdpics', md5(content).hexdigest(), index, '.jpg')\n",
    "        with open(file_path, 'wb') as f: \n",
    "            f.write(content) \n",
    "            print('当前页面爬取完成')\n",
    "\n",
    "# 构造一个可以变化的URL，并下载\n",
    "def main(i): \n",
    "    url = 'http://jandan.net/ooxx/page-'+str(i)+'#comments' \n",
    "    html = get_one_html(url) \n",
    "    urls = parse_html(html)\n",
    "    download_imgs(urls) \n",
    "    \n",
    "# 实现一个多进程爬虫 \n",
    "if __name__ == '__main__':\n",
    "    # for i in range(1, 85):\n",
    "    #     url = 'http://jandan.net/ooxx/page-'+str(i)+'#comments'\n",
    "    #     main(url)\n",
    "    pool = Pool()\n",
    "    pool.map(main, [i for i in range(1, 85)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "word = \"桌面\"\n",
    "\n",
    "url = \"https://image.baidu.com/search/index?tn=baiduimage&ipn=r&ct=201326592\\\n",
    "&cl=2&lm=-1&st=-1&fm=result&fr=&sf=1&fmq=1540822430688_R&pv=&ic=0&nc=1&z=&se=1\\\n",
    "&showtab=0&fb=0&width=&height=&face=0&istype=2&ie=utf-8&word=\" + word\n",
    "\n",
    "header = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:63.0) \\\n",
    "Gecko/20100101 Firefox/63.0\"}\n",
    "\n",
    "browser = webdriver.Chrome() # 或者使用Firefox驱动 webdriver.Firefox()\n",
    "browser.get(url)\n",
    "browser.implicitly_wait(10)\n",
    "\n",
    "i = 0\n",
    "for i in range(5):# 设置下拉5次\n",
    "    browser.execute_script('window.scrollTo(0, document.body.scrollHeight)')# 下拉到最底部\n",
    "    time.sleep(5)# 防止网络延迟，卡顿等\n",
    "\n",
    "r = browser.page_source\n",
    "browser.close()\n",
    "print(r)\n",
    "\n",
    "img_list = re.findall(r'\"objURL\":\"(.*?)\",', r)\n",
    "print(img_list)\n",
    "\n",
    "for img in img_list:\n",
    "    print(img)\n",
    "\n",
    "'''\n",
    "    path = re.sub('\\?|\\/', '', img[-10:])\n",
    "\n",
    "    try:\n",
    "        with open('c:/Temp/' + word +'/{}'.format(path), 'ab') as f:\n",
    "            ret = requests.get(img, headers=header, timeout=30)\n",
    "            f.write(ret.content)\n",
    "    except Exception:\n",
    "        pass\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查询中标公告 旧版网站\n",
    "\n",
    "import requests\n",
    "import re\n",
    "from lxml import etree\n",
    "\n",
    "class ZbSpider(object):\n",
    "    def __init__(self):\n",
    "        self.url = \"http://www.ccgp-qinghai.gov.cn/jilin/zbxxController.form?\"\n",
    "        self.headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:63.0) \\\n",
    "Gecko/20100101 Firefox/63.0\"}\n",
    "        self.page = 0\n",
    "        \n",
    "    def start_request(self):\n",
    "        lx = input(\"请输入公告类型（1.省级 2.州市县级）：\")\n",
    "        pg = input(\"请输入爬取页数：\")\n",
    "        for self.page in range(int(pg)):\n",
    "            page = self.page + 1\n",
    "            print(\"正在爬取中标公告第【%d】页...-----------------------------\" % page)\n",
    "            html = requests.get(self.url+\"declarationType=W\"+\"&type=\"+lx+\"&pageNo=\"+\n",
    "str(self.page), headers=self.headers).content.decode()\n",
    "            self.zbxx(html)\n",
    "        '''\n",
    "        # 废流标公告信息\n",
    "        self.page = 0\n",
    "        for self.page in range(int(pg)):\n",
    "            print(\"正在爬取废流标公告第【%d】页...---------------------------\" % self.page)\n",
    "            html = requests.get(self.url+\"declarationType=F\"+\"&type=\"+lx+\"&pageNo=\"+\n",
    "str(self.page), headers=self.headers).content.decode()\n",
    "            self.zbxx(html)\n",
    "        '''\n",
    "    \n",
    "    #中标公告信息获取\n",
    "    def zbxx(self, html):\n",
    "        #stitle = re.findall(r'title=\"(.*?)\"', html)    #公告标题\n",
    "        surl = re.findall(r'\" href=\"(.*?)\">', html)\n",
    "        for su in surl:\n",
    "            #实际公告页面在框架内，去除框架页多余字符，合成实际url\n",
    "            sggurl = su.replace('ftl/jilin/noticeDetail.jsp?htmlURL=','')\n",
    "            sgg = requests.get(sggurl, headers=self.headers).content\n",
    "            s = etree.HTML(sgg)\n",
    "            name = s.xpath('/html/body/div/table/tbody/tr[2]/td[2]/p/span/text()')\n",
    "            date = s.xpath('/html/body/div/table/tbody/tr[8]/td[2]/p/span/text()')\n",
    "            man = s.xpath('/html/body/div/table/tbody/tr[13]/td[2]/p/span/text()')\n",
    "            print('【'+date[0]+'】',name[0],'【'+man[0]+'】')\n",
    "            print(sggurl)    #打印公告链接\n",
    "if __name__ == \"__main__\":\n",
    "    zb = ZbSpider()\n",
    "    zb.start_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#查询中标公告 V1.0\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import html\n",
    "\n",
    "class ZbSpider(object):\n",
    "    \n",
    "    url = \"http://www.ccgp-qinghai.gov.cn/es-articles/es-article/_search\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:63.0) \\\n",
    "Gecko/20100101 Firefox/63.0\", \"Content-Type\": \"application/json\"} # 提交查询类型为application/json\n",
    "    # Payload查询参数json\n",
    "    payloadData = {\n",
    "        \"from\": 0,\n",
    "        \"size\": 1,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"term\": {\n",
    "                            \"siteId\": {\n",
    "                                \"value\": \"38\",\n",
    "                                \"boost\": 1\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"wildcard\": {\n",
    "                            \"path\": {\n",
    "                                \"wildcard\": \"*6zcyannouncement46*\",\n",
    "                                \"boost\": 1\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"adjust_pure_negative\": True,\n",
    "                \"boost\": 1,\n",
    "                \"should\": []\n",
    "            }\n",
    "        },\n",
    "        \"sort\": [\n",
    "            {\n",
    "                \"publishDate\": {\n",
    "                    \"order\": \"desc\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"_id\": {\n",
    "                    \"order\": \"desc\"\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"_source\": {\n",
    "            \"includes\": [\n",
    "                \"title\",\n",
    "                \"articleId\",\n",
    "                \"siteId\",\n",
    "                \"cover\",\n",
    "                \"url\",\n",
    "                \"pathName\",\n",
    "                \"publishDate\",\n",
    "                \"attachmentUrl\",\n",
    "                \"districtName\",\n",
    "                \"gpCatalogName\"\n",
    "            ],\n",
    "            \"excludes\": [\n",
    "                \"content\"\n",
    "            ]\n",
    "        }\n",
    "    }        \n",
    "    zbgg_addrs = []\n",
    "    \n",
    "    def __init__(self, dfrom, dsize):\n",
    "        self.payloadData['from'] = dfrom # 中标公告列表起始位置\n",
    "        self.payloadData['size'] = dsize # 总数\n",
    " \n",
    "    # 获取中标公告列表   \n",
    "    def zbgg_list(self): \n",
    "        html_text = requests.post(self.url, data=json.dumps(self.payloadData), headers=self.headers)\n",
    "        #print(html_text)\n",
    "        d = json.loads(html_text.content.decode())\n",
    "        size = self.payloadData['size']\n",
    "        for i in range(size):\n",
    "            self.zbgg_addrs.append(\"http://www.ccgp-qinghai.gov.cn\"+d[ 'hits']['hits'][i]['_source']['url']) # 获取中标公告页面地址\n",
    "            # d[ 'hits']['hits'][i]['_source']['title'] 中标公告标题，\n",
    "            # d[ 'hits']['hits'][i]['_source']['url'] 中标公告页面路径\n",
    "\n",
    "    # 打印中标公告信息    \n",
    "    def zbgg_print(self): \n",
    "        i = 1\n",
    "        for zbgg_adddr in self.zbgg_addrs:\n",
    "            html_text = requests.get(zbgg_adddr, headers=self.headers, timeout=50).content.decode()\n",
    "            html_text = html.unescape(html_text) # 反转义\n",
    "            #pbzj = []\n",
    "            #pbzj = re.findall(r'code-85005\\\\\">(.*?)<', html_text) # 匹配评审专家名单\n",
    "            #if pbzj == [] :\n",
    "            pbzj = re.findall(r'名单：(.*?)十', html_text) # 匹配不规范页面中的评审专家名单\n",
    "            pattern = re.compile(u\"[\\u4e00-\\u9fa5、（）]\") # 提取专家名单，保留汉字和\"、（）\"等符号，去除多余字符\n",
    "            pbzj[0] = \"\".join(pattern.findall(pbzj[0])) # 连接成专家名单字符串\n",
    "            cjrq = re.findall(r'code-94002\\\\\">(.*?)<', html_text) # 匹配成交日期\n",
    "            xmmc = re.findall(r'code-00003\\\\\">(.*?)<', html_text) # 匹配项目名称\n",
    "            xmbh = re.findall(r'code-00004\\\\\">(.*?)<', html_text) # 匹配项目编号\n",
    "            print(\"[\"+str(i)+\"] \"+cjrq[0]+\", \"+xmbh[0]+\", \"+xmmc[0]+\", \"+pbzj[0])\n",
    "            i = i + 1\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    zb = ZbSpider(0,100)\n",
    "    zb.zbgg_list()\n",
    "    zb.zbgg_print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 2020-03-13, 青海诚德公招（货物）2020-005, 2020年普通动物疫病疫苗采购项目, 杜银忠、师玉宝、陆艳、陈雷、王生祥、姜立红、段海军\n",
      "[2] 2020-03-12, 青海诚德公招（货物）2020-004, 2020年全省强制免疫疫苗采购项目, 贺晓龙、李建平、沈艳丽、王生祥、赵廷贵、姜立红、段海军\n",
      "[3] 2020-03-13, 青海省招公招（货物）2020-002,  甘德县民政局2020年春季疫情期间救灾粮食政府公开招标采购项目, 沈秀英（组长）、赵海霞、芦平、魏学庆、周鹏（招标人代表）\n",
      "[4] 2020-03-13, 青海汇海源磋商（服务）2020-001, 城西区食品药品安全监管平台升级改造项目, 马永芳（组长）、韩青秀、张峰\n",
      "[5] 2020-03-10, 青海诚鑫竞磋（服务）2020-007, 三江源民族中学二期建设项目校园景观、变配电工程监理, 韦金元、李平、吴威海\n",
      "[6] 2020-03-10, 青海创鑫竞磋（货物）2020-002,  四川喇荣五明佛学院玉树籍尼姑（觉姆）囊谦集中安置点食堂设备采购项目, 郑斌（组长）、李江春、杨涛（采购人代表）\n",
      "[7] 2020-03-10, 川招青海磋商（工程）2020-008, 青海省疾病预防控制中心污水处理站改造项目, 唐渝青（组长）、闫玉霞、闫刚（采购人代表）\n",
      "[8] 2020-03-06, 青海诚德竞磋（服务）2020-001, “金审三期”建设项目监理服务采购, 自行选定专家洪恺、刘睿采购人代表王东立\n",
      "[9] 2020-03-03, 青海国焱竞磋（服务）2020-010号, 青海省妇幼保健院迁址新建项目工程结算审计采购项目, 李宁（组长）、唐渝青和王宗海\n",
      "[10] 2020-03-02, 青海省招竞磋（货物）2020-003, 某部队通信设备及卫生物资采购项目（包三）, 赵宏（组长）、秦振虎、成大华\n",
      "[11] 2020-03-02, 青海省招竞磋（货物）2020-003, 某部队通信设备及卫生物资采购项目（包一）, 赵宏（组长）、秦振虎、成大华\n",
      "[12] 2020-01-22, 青海百鑫公招（货物）2019-138, 青海省第五人民医院功能科多普勒超声波诊断仪采购项目（第四次）, 刘海龙、余兰元、宋国胜、李广斌、谢录玲（采购人代表）\n",
      "[13] 2020-01-21, 青海闫建公招（货物）2019-003, 都兰县2019年农田防护林采伐更新苗木项目, 吴延明（组长）、白生贵、李国发、徐世红、金青龙\n",
      "[14] 2020-01-21, 青海旺利欣竞磋（工程）2020-003号, 青海大学附属医院原急诊内科病区改造采购项目, 胡国元、海玉龙、翟宁田\n",
      "[15] 2020-01-20, 青海增润磋商（服务）2020-02, 城中区七一路348号（原西宁宾馆）省委办公区小礼堂维修改造设计项目, 张海红（组长）阿俊林马亮（业主代表）\n",
      "[16] 2020-01-20, 青海增润磋商（服务）2020-01, 胜利路66号办公楼维修改造设计项目, 李善庆（组长）、李强、马亮（业主代表）\n",
      "[17] 2020-01-20, 青政采公招（货物）2019-345-1号, 青海省妇女儿童医院南院区信息机房设备采购项目, 张志明任继涛梅守刚费发樱牛宁\n",
      "[18] 2020-01-19, 中金（青海）公招（货物）2019-051, 青海省疾病预防控制中心采购2019年结核病检测设备、实验室耗材、试剂等采购项目, 徐文生评委组长）、郭晓阳、赵海星、李明、王朝才（采购人代表）\n",
      "[19] 2020-01-17, 国采（青海）公招（货物）2019-123号, 青海省交通科学研究院交通环境信息平台及在线监测站点建设项目（第三次）, 王锐、谈有恒、杨向军、袁兆林及业主代表杨洪福\n",
      "[20] 2020-01-16, 青政采单招（服务）2019-455号, 青海省青海湖景区保护利用管理局运转服务采购项目, 赵平雷富有马平\n",
      "[21] 2020-01-16, 青海燕达竞谈（工程）2020-001, 2019年壮大村集体经济项目（湟中县共和镇石城村酩馏酒坊项目）, 蒲晓秋（组长）、郭红霞、白彦林（甲方代表）\n",
      "[22] 2020-01-15, 青政采公招（货物）2019-416号, 青海省疾病预防控制中心职业病防治设备采购项目, 黄湘宁徐小艳赵四海刘新刘治华\n",
      "[23] 2020-01-13, 丰汇国际公招（货物）2019-119, 青海省消防救援总队2019年度第四次装备采购项目（第二次）, 曹金刚（招标人评标代表）、鱼建青（招标人评标代表）、张庆军、许新华、李世伟、邓影萍、尹士周\n",
      "[24] 2020-01-14, 青海创鑫竞谈（工程）2020-001, 生物园区街景亮化及维修工程, 刘玮萍（组长）、崔茹、葛文俊（甲方代表）\n",
      "[25] 2020-01-14, 青海富旭公招（服务）2019-016, 青海美术馆物业管理服务项目, 刘金梅（组长）、马良吉、曾梅、齐翠兰、陈治元\n",
      "[26] 2020-01-14, 青政采公招（服务）2019-334-1-1号, 国家税务总局青海省税务局社保费及非税收入征管职责划转信息化建设项目（二）, 许卫国杨向军何钰何海炜柴顺君\n",
      "[27] 2020-01-13, 中金（青海）竞磋（货物）2019-050 , 青海省疾病预防控制中心2019年结核病防治项目实验室耗材、试剂采购项目, 张恩源（组长）、王金柱、王朝才（采购人代表）\n",
      "[28] 2020-01-13, 青海省招公招（服务）2019-045, 2020年青海省图书馆物业服务项目（二次）, 刘伟莉、娄维国（组长）、曹玉芹、李青芳、邵新华（采购人代表）\n",
      "[29] 2020-01-13, 青政采磋商（货物）2019-413号, 青海省传染病专科医院智能化建设采购项目, 张波朱青松许杰\n",
      "[30] 2020-01-10, 青海鸿阳公招（服务）2019-058-1, 重点污染源排放监管及绩效评估项目（第二次）, 赵海梅、艾光泽、黄勇、王书华及业主代表马强\n",
      "[31] 2020-01-10, 青海省招竞磋（工程）2019-052, 青海多巴国家高原体育训练基地重点区域绿化项目（二次）, 王永栋（组长）、陈生辉（采购人代表）、朱永红、赵旭、张永苏\n",
      "[32] 2020-01-10, 华新青海磋商（货物）2019-096号, 青海省第五人民医院办公耗材及保养维修外包采购项目, 雷富有（组长）、李玲、梁祎（业主）\n",
      "[33] 2020-01-09, 青海诚德公招（货物）2019-275, 粮食质量安全检验监测体系建设设备采购项目（第二次）, 薛志斌、白建青、彭巍、周蔚、轩春江\n",
      "[34] 2020-01-09, 青海鼎誉公招（货物）2019-083, 青海省血液中心进口大容量低温离心机采购项目（第二次, 张亮、方靖、姜双应、冯秀娟、杨静（采购人代表）\n",
      "[35] 2020-01-09, 丰汇国际公招（货物）2019-142/01-02, 应急通信装备及卫星通信指挥车采购项目, 黄湘宁（招标人评标代表）、徐峰（招标人评标代表）、许卫国、梅守荣、李发强、朱青松、章鸣\n",
      "[36] 2020-01-08, 青海来瑞竟磋（服务）2019-22号, 2018年林业改革发展及林业生态保护恢复资金青海柴达木梭梭林国家级自然保护区建设项目, 组长赵永来组员刘秀凤、褚建\n",
      "[37] 2020-01-09, 青海省招公招（货物）2019-055, 同德县2019年中央财政支持乡镇卫生院临床服务能力建设项目（五乡镇卫生院医疗设备采购项目）, 拉本加（采购人代表）、赵生虎（组长）、冯书娥、董军、郑兰芳\n",
      "[38] 2020-01-09, 青政采公招（货物）2019-411号, 青海省公路局皮卡车购置项目, 曹金刚赵建宁薛亦德柳晓华李生登\n",
      "[39] 2020-01-07, 青海诚鑫公招（工程）2019-185, 青海师范大学实验实训基地建设项目, 毛福全、陈守柱、余兰萍、张凯旋、张金萍、黄成辉、李兰\n",
      "[40] 2020-01-08, 青政采公招（货物）2019-433号, 青海省卫生健康委员会卫生监督执法终端采购项目, 郭军生黄鹏张召华费发樱董剑秋\n",
      "[41] 2020-01-06, 四川天信竞磋（货物）2019-056, 囊谦县乡镇卫生院能力提升项目智能家庭服务平台设备（着晓乡、尕羊乡、觉拉乡、吉尼赛乡、吉曲乡）, \n",
      "[42] 2020-01-07,  四川天信竞磋（工程）2019 -055 , 囊谦县东坝乡卫生院标准化建设项目, \n",
      "[43] 2020-01-06, 丰汇国际公招（货物）2019-001/011, 青海省消防救援总队2019年度第二次装备采购项目（第11包）（第四次）, 熊建国（招标人评标代表）、王渭清、许新华、王跃忠、杨小霞\n",
      "[44] 2020-01-03, 青海百鑫公招（货物）2019-127, 青海省公路局海东公路总段2019年公路养护小型机具采购项目（第二次）, 李秉科、赵建宁、卢敏、娄维国、顾声洁\n",
      "[45] 2020-01-07, 青政采公招（服务）2019-423号, 青海省疾病预防控制中心免疫规划信息管理系统采购项目, 李明郑国贤徐岩曹志林阿克忠\n",
      "[46] 2020-01-07, 青海汭辉竞磋（货物）2019-036, 青海省建新监狱商品供应项目, 组长杨小萍成员杨小萍、张启芳、符建东\n",
      "[47] 2020-01-07, 川招青海公招（货物）2019-230L, 青海大学附属医院检验科设备采购项目(第二次), 方靖（组长）、郭晓阳、李启光、杜建梅、赵玲莉（采购方代表）\n",
      "[48] 2020-01-07, 青海诚德公招（服务）2019-272, 2019年度重点环境风险源预警监控运行维护项目, 马锡铎、雷富有、许卫国、袁兆林、王聪\n",
      "[49] 2020-01-06, 湖南中誉竞磋（货物）2019-003, 同德县巴沟乡班多村乡村振兴产业发展标准化养殖区建设项目, 李文刚（业主）、王业伟、周鹏\n",
      "[50] 2020年01月06日, 青海品冠竞磋（货物）2019-078, 官亭中学校园广播系统项目, 张同宁、李发强、朱学林\n"
     ]
    }
   ],
   "source": [
    "#查询中标公告 V2.0\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import html\n",
    "\n",
    "class ZbSpider(object):\n",
    "    \n",
    "    url = \"http://www.ccgp-qinghai.gov.cn/front/search/category\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:63.0) \\\n",
    "Gecko/20100101 Firefox/63.0\", \"Content-Type\": \"application/json\"} # 提交查询类型为application/json\n",
    "    # Payload查询参数json\n",
    "    payloadData = {\n",
    "        \"pageNo\":1,\n",
    "        \"pageSize\":1,\n",
    "        \"categoryCode\":\"ZcyAnnouncement4\",\n",
    "        \"districtCode\":[\"639900\"]\n",
    "    }        \n",
    "    zbgg_addrs = []\n",
    "    \n",
    "    def __init__(self, dfrom, dsize):\n",
    "        self.payloadData['pageNo'] = dfrom # 中标公告列表起始位置\n",
    "        self.payloadData['pageSize'] = dsize # 总数\n",
    " \n",
    "    # 获取中标公告列表   \n",
    "    def zbgg_list(self): \n",
    "        html_text = requests.post(self.url, data=json.dumps(self.payloadData), headers=self.headers)\n",
    "        d = json.loads(html_text.content.decode())\n",
    "        size = self.payloadData['pageSize']\n",
    "        for i in range(size):\n",
    "            self.zbgg_addrs.append(\"http://www.ccgp-qinghai.gov.cn\"+d[ 'hits']['hits'][i]['_source']['url']) # 获取中标公告页面地址\n",
    "            # d[ 'hits']['hits'][i]['_source']['title'] 中标公告标题，\n",
    "            # d[ 'hits']['hits'][i]['_source']['url'] 中标公告页面路径\n",
    "\n",
    "    # 打印中标公告信息    \n",
    "    def zbgg_print(self): \n",
    "        i = 1\n",
    "        for zbgg_adddr in self.zbgg_addrs:\n",
    "            html_text = requests.get(zbgg_adddr, headers=self.headers, timeout=50).content.decode()\n",
    "            html_text = html.unescape(html_text) # 反转义\n",
    "            #pbzj = re.findall(r'code-85005\\\\\">(.*?)<', html_text) # 匹配评审专家名单\n",
    "            pbzj = re.findall(r'名单：(.*?)十', html_text) # 匹配不规范页面中的评审专家名单\n",
    "            pattern = re.compile(u\"[\\u4e00-\\u9fa5、（）]\") # 提取专家名单，保留汉字和\"、（）\"等符号，去除多余字符\n",
    "            pbzj[0] = \"\".join(pattern.findall(pbzj[0])) # 连接成专家名单字符串\n",
    "            cjrq = re.findall(r'code-94002 date-selection-cls\\\\\">(.*?)<', html_text) # 匹配成交日期\n",
    "            xmmc = re.findall(r'code-00003 single-line-text-input-box-cls\\\\\">(.*?)<', html_text) # 匹配项目名称\n",
    "            xmbh = re.findall(r'code-00004 single-line-text-input-box-cls\\\\\">(.*?)<', html_text) # 匹配项目编号\n",
    "            print(\"[\"+str(i)+\"] \"+cjrq[0]+\", \"+xmbh[0]+\", \"+xmmc[0]+\", \"+pbzj[0])\n",
    "            i = i + 1\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    zb = ZbSpider(1,50)\n",
    "    zb.zbgg_list()\n",
    "    zb.zbgg_print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _*_ coding: utf-8 _*_\n",
    "# 学习强国\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "__author__ = 'Silent_Coder'\n",
    "__date__ = '2019/3/12 22:41'\n",
    "\n",
    "HOME_PAGE = 'https://www.xuexi.cn/'\n",
    "VIDEO_LINK = 'https://www.xuexi.cn/a191dbc3067d516c3e2e17e2e08953d6/b87d700beee2c44826a9202c75d18c85.html?pageNumber=39'\n",
    "LONG_VIDEO_LINK = 'https://www.xuexi.cn/f65dae4a57fe21fcc36f3506d660891c/b2e5aa79be613aed1f01d261c4a2ae17.html'\n",
    "LONG_VIDEO_LINK2 = 'https://www.xuexi.cn/0040db2a403b0b9303a68b9ae5a4cca0/b2e5aa79be613aed1f01d261c4a2ae17.html'\n",
    "TEST_VIDEO_LINK = 'https://www.xuexi.cn/8e35a343fca20ee32c79d67e35dfca90/7f9f27c65e84e71e1b7189b7132b4710.html'\n",
    "SCORES_LINK = 'https://pc.xuexi.cn/points/my-points.html'\n",
    "LOGIN_LINK = 'https://pc.xuexi.cn/points/login.html'\n",
    "ARTICLES_LINK = 'https://www.xuexi.cn/d05cad69216e688d304bb91ef3aac4c6/9a3668c13f6e303932b5e0e100fc248b.html'\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "browser = webdriver.Chrome(options=options)\n",
    "\n",
    "\n",
    "def login_simulation():\n",
    "    \"\"\"模拟登录\"\"\"\n",
    "    # 方式一：使用cookies方式\n",
    "    # 先自己登录，然后复制token值覆盖\n",
    "    # cookies = {'name': 'token', 'value': ''}\n",
    "    # browser.add_cookie(cookies)\n",
    "\n",
    "    # 方式二：自己扫码登录\n",
    "    browser.get(LOGIN_LINK)\n",
    "    browser.maximize_window()\n",
    "    browser.execute_script(\"var q=document.documentElement.scrollTop=1000\")\n",
    "    time.sleep(10)\n",
    "    browser.get(HOME_PAGE)\n",
    "    print(\"模拟登录完毕\\n\")\n",
    "\n",
    "\n",
    "def watch_videos():\n",
    "    \"\"\"观看视频\"\"\"\n",
    "    browser.get(VIDEO_LINK)\n",
    "    videos = browser.find_elements_by_xpath(\"//div[@id='Ck3ln2wlyg3k00']\")\n",
    "    spend_time = 0\n",
    "\n",
    "    for i, video in enumerate(videos):\n",
    "        if i > 6:\n",
    "            break\n",
    "        video.click()\n",
    "        all_handles = browser.window_handles\n",
    "        browser.switch_to_window(all_handles[-1])\n",
    "        browser.get(browser.current_url)\n",
    "\n",
    "        # 点击播放\n",
    "        browser.find_element_by_xpath(\"//div[@class='outter']\").click()\n",
    "        # 获取视频时长\n",
    "        video_duration_str = browser.find_element_by_xpath(\"//span[@class='duration']\").get_attribute('innerText')\n",
    "        video_duration = int(video_duration_str.split(':')[0]) * 60 + int(video_duration_str.split(':')[1])\n",
    "        # 保持学习，直到视频结束\n",
    "        time.sleep(video_duration + 3)\n",
    "        spend_time += video_duration + 3\n",
    "        browser.close()\n",
    "        browser.switch_to_window(all_handles[0])\n",
    "\n",
    "    # if spend_time < 3010:\n",
    "    #     browser.get(LONG_VIDEO_LINK)\n",
    "    #     browser.execute_script(\"var q=document.documentElement.scrollTop=850\")\n",
    "    #     try:\n",
    "    #         browser.find_element_by_xpath(\"//div[@class='outter']\").click()\n",
    "    #     except:\n",
    "    #         pass\n",
    "    #\n",
    "    #     # 观看剩下的时间\n",
    "    #     time.sleep(3010 - spend_time)\n",
    "    browser.get(TEST_VIDEO_LINK)\n",
    "    time.sleep(3010 - spend_time)\n",
    "    print(\"播放视频完毕\\n\")\n",
    "\n",
    "\n",
    "def read_articles():\n",
    "    \"\"\"阅读文章\"\"\"\n",
    "    browser.get(ARTICLES_LINK)\n",
    "    articles = browser.find_elements_by_xpath(\"//div[@id='Ca4gvo4bwg7400']\")\n",
    "    for index, article in enumerate(articles):\n",
    "        if index > 7:\n",
    "            break\n",
    "        article.click()\n",
    "        all_handles = browser.window_handles\n",
    "        browser.switch_to_window(all_handles[-1])\n",
    "        browser.get(browser.current_url)\n",
    "        for i in range(0, 2000, 100):\n",
    "\n",
    "            js_code = \"var q=document.documentElement.scrollTop=\" + str(i)\n",
    "            browser.execute_script(js_code)\n",
    "            time.sleep(5)\n",
    "        for i in range(2000, 0, -100):\n",
    "            js_code = \"var q=document.documentElement.scrollTop=\" + str(i)\n",
    "            browser.execute_script(js_code)\n",
    "            time.sleep(5)\n",
    "        time.sleep(80)\n",
    "        browser.close()\n",
    "        browser.switch_to_window(all_handles[0])\n",
    "    print(\"阅读文章完毕\\n\")\n",
    "\n",
    "\n",
    "def get_scores():\n",
    "    \"\"\"获取当前积分\"\"\"\n",
    "    browser.get(SCORES_LINK)\n",
    "    time.sleep(2)\n",
    "    gross_score = browser.find_element_by_xpath(\"//*[@id='app']/div/div[2]/div/div[2]/div[2]/span[1]\")\\\n",
    "        .get_attribute('innerText')\n",
    "    today_score = browser.find_element_by_xpath(\"//span[@class='my-points-points']\").get_attribute('innerText')\n",
    "    print(\"当前总积分：\" + str(gross_score))\n",
    "    print(\"今日积分：\" + str(today_score))\n",
    "    print(\"获取积分完毕，即将退出\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    login_simulation()  # 模拟登录\n",
    "    read_articles()     # 阅读文章\n",
    "    watch_videos()      # 观看视频\n",
    "    get_scores()        # 获得今日积分\n",
    "    browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
