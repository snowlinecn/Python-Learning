{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "\n",
    "word = input(\"请输入要搜索的图片：\")\n",
    "if not os.path.exists('c:/Temp/' + word):\n",
    "    os.mkdir('c:/Temp/' + word)\n",
    "\n",
    "url = \"https://image.baidu.com/search/index?tn=baiduimage&ipn=r&ct=201326592\\\n",
    "&cl=2&lm=-1&st=-1&fm=result&fr=&sf=1&fmq=1540822430688_R&pv=&ic=0&nc=1&z=&se=1\\\n",
    "&showtab=0&fb=0&width=&height=&face=0&istype=2&ie=utf-8&word=\" + word\n",
    "\n",
    "header = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:63.0) \\\n",
    "Gecko/20100101 Firefox/63.0\"}\n",
    "\n",
    "r = requests.get(url, headers=header, timeout=3).content.decode(\"utf-8\")\n",
    "\n",
    "img_list = re.findall(r'\"objURL\":\"(.*?)\",', r)\n",
    "\n",
    "for img in img_list:\n",
    "    print(img)\n",
    "    end = re.search(r'(.jpg|.png|.gif|.jpeg)$', img)\n",
    "    if end == None:\n",
    "        img = img +'.jpg'\n",
    "\n",
    "    path = re.sub('\\?|\\/', '', img[-10:])\n",
    "\n",
    "    try:\n",
    "        with open('c:/Temp/' + word +'/{}'.format(path), 'ab') as f:\n",
    "            ret = requests.get(img, headers=header, timeout=3)\n",
    "            f.write(ret.content)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _md5 import md5\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# 获取完整网页源码 \n",
    "def get_one_html(url): \n",
    "    browser = webdriver.Phantomjs()\n",
    "    browser.get(url) \n",
    "    p_source = browser.page_source\n",
    "    browser.close() \n",
    "    return p_source \n",
    "\n",
    "# 用正则表达式提取链接并且返回URL \n",
    "def parse_html(html): \n",
    "    pattern = re.compile(r'<li.*?<img src=\"(.*?)\".*?/>.*?</li>', re.S)\n",
    "    urls = re.findall(pattern, html)\n",
    "    print(urls)\n",
    "    return urls\n",
    "\n",
    "# 下载图片，注意的是文件写入的是二进制，并且文件名使用MD5生成固定长度的十六进制，防止文件名冲突，index表示页数 \n",
    "def download_imgs(urls):\n",
    "    print('one page')\n",
    "    for index, url in enumerate(urls): \n",
    "        response = requests.get(url) \n",
    "        content = response.content\n",
    "        file_path = '{0}\\\\{1}{2}{3}'.format('E:\\\\jdpics', md5(content).hexdigest(), index, '.jpg')\n",
    "        with open(file_path, 'wb') as f: \n",
    "            f.write(content) \n",
    "            print('当前页面爬取完成')\n",
    "\n",
    "# 构造一个可以变化的URL，并下载\n",
    "def main(i): \n",
    "    url = 'http://jandan.net/ooxx/page-'+str(i)+'#comments' \n",
    "    html = get_one_html(url) \n",
    "    urls = parse_html(html)\n",
    "    download_imgs(urls) \n",
    "    \n",
    "# 实现一个多进程爬虫 \n",
    "if __name__ == '__main__':\n",
    "    # for i in range(1, 85):\n",
    "    #     url = 'http://jandan.net/ooxx/page-'+str(i)+'#comments'\n",
    "    #     main(url)\n",
    "    pool = Pool()\n",
    "    pool.map(main, [i for i in range(1, 85)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "word = \"桌面\"\n",
    "\n",
    "url = \"https://image.baidu.com/search/index?tn=baiduimage&ipn=r&ct=201326592\\\n",
    "&cl=2&lm=-1&st=-1&fm=result&fr=&sf=1&fmq=1540822430688_R&pv=&ic=0&nc=1&z=&se=1\\\n",
    "&showtab=0&fb=0&width=&height=&face=0&istype=2&ie=utf-8&word=\" + word\n",
    "\n",
    "header = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:63.0) \\\n",
    "Gecko/20100101 Firefox/63.0\"}\n",
    "\n",
    "browser = webdriver.Firefox()\n",
    "browser.get(url)\n",
    "browser.implicitly_wait(10)\n",
    "\n",
    "i = 0\n",
    "for i in range(5):# 设置下拉5次\n",
    "    browser.execute_script('window.scrollTo(0, document.body.scrollHeight)')# 下拉到最底部\n",
    "    time.sleep(5)# 防止网络延迟，卡顿等\n",
    "\n",
    "r = browser.page_source\n",
    "browser.close()\n",
    "print(r)\n",
    "\n",
    "img_list = re.findall(r'\"objURL\":\"(.*?)\",', r)\n",
    "print(img_list)\n",
    "\n",
    "for img in img_list:\n",
    "    print(img)\n",
    "\n",
    "'''\n",
    "    path = re.sub('\\?|\\/', '', img[-10:])\n",
    "\n",
    "    try:\n",
    "        with open('c:/Temp/' + word +'/{}'.format(path), 'ab') as f:\n",
    "            ret = requests.get(img, headers=header, timeout=30)\n",
    "            f.write(ret.content)\n",
    "    except Exception:\n",
    "        pass\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查询中标公告 旧版网站\n",
    "\n",
    "import requests\n",
    "import re\n",
    "from lxml import etree\n",
    "\n",
    "class ZbSpider(object):\n",
    "    def __init__(self):\n",
    "        self.url = \"http://www.ccgp-qinghai.gov.cn/jilin/zbxxController.form?\"\n",
    "        self.headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:63.0) \\\n",
    "Gecko/20100101 Firefox/63.0\"}\n",
    "        self.page = 0\n",
    "        \n",
    "    def start_request(self):\n",
    "        lx = input(\"请输入公告类型（1.省级 2.州市县级）：\")\n",
    "        pg = input(\"请输入爬取页数：\")\n",
    "        for self.page in range(int(pg)):\n",
    "            page = self.page + 1\n",
    "            print(\"正在爬取中标公告第【%d】页...-----------------------------\" % page)\n",
    "            html = requests.get(self.url+\"declarationType=W\"+\"&type=\"+lx+\"&pageNo=\"+\n",
    "str(self.page), headers=self.headers).content.decode()\n",
    "            self.zbxx(html)\n",
    "        '''\n",
    "        # 废流标公告信息\n",
    "        self.page = 0\n",
    "        for self.page in range(int(pg)):\n",
    "            print(\"正在爬取废流标公告第【%d】页...---------------------------\" % self.page)\n",
    "            html = requests.get(self.url+\"declarationType=F\"+\"&type=\"+lx+\"&pageNo=\"+\n",
    "str(self.page), headers=self.headers).content.decode()\n",
    "            self.zbxx(html)\n",
    "        '''\n",
    "    \n",
    "    #中标公告信息获取\n",
    "    def zbxx(self, html):\n",
    "        #stitle = re.findall(r'title=\"(.*?)\"', html)    #公告标题\n",
    "        surl = re.findall(r'\" href=\"(.*?)\">', html)\n",
    "        for su in surl:\n",
    "            #实际公告页面在框架内，去除框架页多余字符，合成实际url\n",
    "            sggurl = su.replace('ftl/jilin/noticeDetail.jsp?htmlURL=','')\n",
    "            sgg = requests.get(sggurl, headers=self.headers).content\n",
    "            s = etree.HTML(sgg)\n",
    "            name = s.xpath('/html/body/div/table/tbody/tr[2]/td[2]/p/span/text()')\n",
    "            date = s.xpath('/html/body/div/table/tbody/tr[8]/td[2]/p/span/text()')\n",
    "            man = s.xpath('/html/body/div/table/tbody/tr[13]/td[2]/p/span/text()')\n",
    "            print('【'+date[0]+'】',name[0],'【'+man[0]+'】')\n",
    "            print(sggurl)    #打印公告链接\n",
    "if __name__ == \"__main__\":\n",
    "    zb = ZbSpider()\n",
    "    zb.start_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#查询中标公告 V1.0\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import html\n",
    "\n",
    "class ZbSpider(object):\n",
    "    \n",
    "    url = \"http://www.ccgp-qinghai.gov.cn/es-articles/es-article/_search\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:63.0) \\\n",
    "Gecko/20100101 Firefox/63.0\", \"Content-Type\": \"application/json\"} # 提交查询类型为application/json\n",
    "    # Payload查询参数json\n",
    "    payloadData = {\n",
    "        \"from\": 0,\n",
    "        \"size\": 1,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"term\": {\n",
    "                            \"siteId\": {\n",
    "                                \"value\": \"38\",\n",
    "                                \"boost\": 1\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"wildcard\": {\n",
    "                            \"path\": {\n",
    "                                \"wildcard\": \"*6zcyannouncement46*\",\n",
    "                                \"boost\": 1\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"adjust_pure_negative\": True,\n",
    "                \"boost\": 1,\n",
    "                \"should\": []\n",
    "            }\n",
    "        },\n",
    "        \"sort\": [\n",
    "            {\n",
    "                \"publishDate\": {\n",
    "                    \"order\": \"desc\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"_id\": {\n",
    "                    \"order\": \"desc\"\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"_source\": {\n",
    "            \"includes\": [\n",
    "                \"title\",\n",
    "                \"articleId\",\n",
    "                \"siteId\",\n",
    "                \"cover\",\n",
    "                \"url\",\n",
    "                \"pathName\",\n",
    "                \"publishDate\",\n",
    "                \"attachmentUrl\",\n",
    "                \"districtName\",\n",
    "                \"gpCatalogName\"\n",
    "            ],\n",
    "            \"excludes\": [\n",
    "                \"content\"\n",
    "            ]\n",
    "        }\n",
    "    }        \n",
    "    zbgg_addrs = []\n",
    "    \n",
    "    def __init__(self, dfrom, dsize):\n",
    "        self.payloadData['from'] = dfrom # 中标公告列表起始位置\n",
    "        self.payloadData['size'] = dsize # 总数\n",
    " \n",
    "    # 获取中标公告列表   \n",
    "    def zbgg_list(self): \n",
    "        html_text = requests.post(self.url, data=json.dumps(self.payloadData), headers=self.headers)\n",
    "        #print(html_text)\n",
    "        d = json.loads(html_text.content.decode())\n",
    "        size = self.payloadData['size']\n",
    "        for i in range(size):\n",
    "            self.zbgg_addrs.append(\"http://www.ccgp-qinghai.gov.cn\"+d[ 'hits']['hits'][i]['_source']['url']) # 获取中标公告页面地址\n",
    "            # d[ 'hits']['hits'][i]['_source']['title'] 中标公告标题，\n",
    "            # d[ 'hits']['hits'][i]['_source']['url'] 中标公告页面路径\n",
    "\n",
    "    # 打印中标公告信息    \n",
    "    def zbgg_print(self): \n",
    "        i = 1\n",
    "        for zbgg_adddr in self.zbgg_addrs:\n",
    "            html_text = requests.get(zbgg_adddr, headers=self.headers, timeout=50).content.decode()\n",
    "            html_text = html.unescape(html_text) # 反转义\n",
    "            #pbzj = []\n",
    "            #pbzj = re.findall(r'code-85005\\\\\">(.*?)<', html_text) # 匹配评审专家名单\n",
    "            #if pbzj == [] :\n",
    "            pbzj = re.findall(r'名单：(.*?)十', html_text) # 匹配不规范页面中的评审专家名单\n",
    "            pattern = re.compile(u\"[\\u4e00-\\u9fa5、（）]\") # 提取专家名单，保留汉字和\"、（）\"等符号，去除多余字符\n",
    "            pbzj[0] = \"\".join(pattern.findall(pbzj[0])) # 连接成专家名单字符串\n",
    "            cjrq = re.findall(r'code-94002\\\\\">(.*?)<', html_text) # 匹配成交日期\n",
    "            xmmc = re.findall(r'code-00003\\\\\">(.*?)<', html_text) # 匹配项目名称\n",
    "            xmbh = re.findall(r'code-00004\\\\\">(.*?)<', html_text) # 匹配项目编号\n",
    "            print(\"[\"+str(i)+\"] \"+cjrq[0]+\", \"+xmbh[0]+\", \"+xmmc[0]+\", \"+pbzj[0])\n",
    "            i = i + 1\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    zb = ZbSpider(0,100)\n",
    "    zb.zbgg_list()\n",
    "    zb.zbgg_print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 2019-07-08, 川招青海磋商（货物）2019-096, 青海大学实验室改造项目, 黄湘宁（组长）、李世伟、罗军鹏（采购人代表）\n",
      "[2] 2019-07-09, 中海建竞磋（货物）2019-015, 青海省体育工作一大队田径队训练服装器材采购项目, 王焕强（主任委员）、王宇环、张苗（采购人代表）\n",
      "[3] 2019-07-09, 中海建竞磋（货物）2019-014, 青海省体育工作一大队武术队训练服装器材采购项目, 董汇泽（主任委员）、刘彩霞、赵燕（采购人代表）\n",
      "[4] 2019-07-08, 青海明锦竞磋（货物）2019-007, 青海省公路局海东公路总段门源公路段关于采购砂石料的项目, 张小栓（采购人评标代表）、周海明、龙玉萍\n",
      "[5] 2019-07-09, 国采（青海）竞磋（服务）2019-050号, 青海大学附属医院后勤物资供应商遴选项目（第二次）, 吴宝宁、马良吉及业主代表王景文\n",
      "[6] 2019-07-09, 华春磋商（货物）2019-006, 青海省公路局海西公路总段养护中心采购抗裂贴项目, 孟有恒（组长）、赵春爱、杨萌（采购单位代表）\n",
      "[7] 2019-07-08, 青海紫宸竞磋（服务）2019-012号, 青海省交通医院物业服务采购项目, 费胜章、韩海伟、李兴\n",
      "[8] 2019-07-09, 宏业建设竞磋（工程）2019-017, 青海大学三江源生态一流学科(高原农业种质资源创新与利用方向)实验室改造, 党广清（主任委员）、郑建青、滕长才（采购人代表）\n",
      "[9] 2019-07-09, 华新磋商（服务）2019-023号, 开展亚行贷款西宁市森林可持续经营项目研究报告, \n",
      "[10] 2019-07-09, 深圳振东竞磋（货物）2019-022, 2019年职教优势特色专业建设护理系临床护理思维训练系统采购项目, 冯秀娟、郭晓阳、任玉录\n",
      "[11] 2019-07-09, 德汇青海公招（货物）2019-003, 远程医疗平台建设, 贾守宁（组长）、莫小为、谷淑阁、丁东升、范鑫（业主代表）\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bfa45977c8b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mzb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mZbSpider\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mzb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzbgg_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0mzb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzbgg_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-bfa45977c8b7>\u001b[0m in \u001b[0;36mzbgg_print\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mpbzj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'名单：(.*?)十'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhtml_text\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 匹配不规范页面中的评审专家名单\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu\"[\\u4e00-\\u9fa5、（）]\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 提取专家名单，保留汉字和\"、（）\"等符号，去除多余字符\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[0mpbzj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpbzj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 连接成专家名单字符串\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m             \u001b[0mcjrq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'code-94002 date-selection-cls\\\\\">(.*?)<'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhtml_text\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 匹配成交日期\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mxmmc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'code-00003 single-line-text-input-box-cls\\\\\">(.*?)<'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhtml_text\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 匹配项目名称\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#查询中标公告 V2.0\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import html\n",
    "\n",
    "class ZbSpider(object):\n",
    "    \n",
    "    url = \"http://www.ccgp-qinghai.gov.cn/front/search/category\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:63.0) \\\n",
    "Gecko/20100101 Firefox/63.0\", \"Content-Type\": \"application/json\"} # 提交查询类型为application/json\n",
    "    # Payload查询参数json\n",
    "    payloadData = {\n",
    "        \"pageNo\":1,\n",
    "        \"pageSize\":1,\n",
    "        \"categoryCode\":\"ZcyAnnouncement4\",\n",
    "        \"districtCode\":[\"639900\"]\n",
    "    }        \n",
    "    zbgg_addrs = []\n",
    "    \n",
    "    def __init__(self, dfrom, dsize):\n",
    "        self.payloadData['pageNo'] = dfrom # 中标公告列表起始位置\n",
    "        self.payloadData['pageSize'] = dsize # 总数\n",
    " \n",
    "    # 获取中标公告列表   \n",
    "    def zbgg_list(self): \n",
    "        html_text = requests.post(self.url, data=json.dumps(self.payloadData), headers=self.headers)\n",
    "        d = json.loads(html_text.content.decode())\n",
    "        size = self.payloadData['pageSize']\n",
    "        for i in range(size):\n",
    "            self.zbgg_addrs.append(\"http://www.ccgp-qinghai.gov.cn\"+d[ 'hits']['hits'][i]['_source']['url']) # 获取中标公告页面地址\n",
    "            # d[ 'hits']['hits'][i]['_source']['title'] 中标公告标题，\n",
    "            # d[ 'hits']['hits'][i]['_source']['url'] 中标公告页面路径\n",
    "\n",
    "    # 打印中标公告信息    \n",
    "    def zbgg_print(self): \n",
    "        i = 1\n",
    "        for zbgg_adddr in self.zbgg_addrs:\n",
    "            html_text = requests.get(zbgg_adddr, headers=self.headers, timeout=50).content.decode()\n",
    "            html_text = html.unescape(html_text) # 反转义\n",
    "            #pbzj = re.findall(r'code-85005\\\\\">(.*?)<', html_text) # 匹配评审专家名单\n",
    "            pbzj = re.findall(r'名单：(.*?)十', html_text) # 匹配不规范页面中的评审专家名单\n",
    "            pattern = re.compile(u\"[\\u4e00-\\u9fa5、（）]\") # 提取专家名单，保留汉字和\"、（）\"等符号，去除多余字符\n",
    "            pbzj[0] = \"\".join(pattern.findall(pbzj[0])) # 连接成专家名单字符串\n",
    "            cjrq = re.findall(r'code-94002 date-selection-cls\\\\\">(.*?)<', html_text) # 匹配成交日期\n",
    "            xmmc = re.findall(r'code-00003 single-line-text-input-box-cls\\\\\">(.*?)<', html_text) # 匹配项目名称\n",
    "            xmbh = re.findall(r'code-00004 single-line-text-input-box-cls\\\\\">(.*?)<', html_text) # 匹配项目编号\n",
    "            print(\"[\"+str(i)+\"] \"+cjrq[0]+\", \"+xmbh[0]+\", \"+xmmc[0]+\", \"+pbzj[0])\n",
    "            i = i + 1\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    zb = ZbSpider(1,50)\n",
    "    zb.zbgg_list()\n",
    "    zb.zbgg_print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
